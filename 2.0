import requests
from lxml import html
from requests.adapters import HTTPAdapter
import time
import random
import logging
from urllib.parse import urlparse
from urllib.robotparser import RobotFileParser

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
]

session = requests.Session()
adapter = HTTPAdapter(max_retries=3)
session.mount("http://", adapter)
session.mount("https://", adapter)

def can_fetch(url):
    rp = RobotFileParser()
    rp.set_url(f"{urlparse(url).scheme}://{urlparse(url).netloc}/robots.txt")
    rp.read()
    #return rp.can_fetch("*", url)
    return True

def fetch_with_retry(url, params, headers, max_retries=3):
    for attempt in range(max_retries):
        try:
            if not can_fetch(url):
                logger.warning(f"Robots.txt disallows fetching {url}")
                return None
            response = session.get(url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            logger.error(f"Request failed on attempt {attempt + 1}/{max_retries}. Error: {e}")
            if attempt < max_retries - 1:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                logger.info(f"Retrying in {wait_time:.2f} seconds...")
                time.sleep(wait_time)
    return None

def fetch_google_search_results(query, num_results=10, num_pages=1):
    all_links = []
    search_url = "https://www.google.com/search"
    start = 0
    headers = {'User-Agent': random.choice(USER_AGENTS)}

    while len(all_links) < num_results and start < num_results * num_pages:
        params = {
            "q": query,
            "num": num_results,
            "start": start
        }
        try:
            response = fetch_with_retry(search_url, params, headers)
            if response is None:
                break
            logger.info(f"Response status code: {response.status_code}")
            logger.info(f"Response content length: {len(response.text)}")

            tree = html.fromstring(response.content)
            links = extract_search_result_links(tree)
            logger.info(f"Found {len(links)} links on this page")
            all_links.extend(links)
            start += num_results
            if not links:
                logger.info("No links found on this page. Breaking the loop.")
                break

            time.sleep(random.uniform(1, 3))  # Random delay between requests
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            break
    return all_links[:num_results]

def extract_search_result_links(tree):
    links = []
    for element in tree.xpath('//div[@class="yuRUbf"]//a[@href] | //a[@class="WlydOe"]'):
        url = element.get('href')
        parsed_url = urlparse(url)
        if parsed_url.scheme in ['http', 'https'] and is_valid_domain(parsed_url.netloc):
            links.append(url)
    return links

def is_valid_domain(domain):
    disallowed_domains = ['maps.google.com', 'support.google.com', 'accounts.google.com',
                          'www.linkedin.com', 'www.youtube.com']
    return domain not in disallowed_domains

def main():
    name = "Ravi Kumar S"
    role = "CEO"
    company = "Cognizant"
    details = "Biography"
    industry = "Information Technology & Services"
    query = f"{name} {role} {company} {industry} {details}"
    logger.info(f"Search query: {query}")
    num_results = 10
    num_pages = 2

    search_results = fetch_google_search_results(query, num_results, num_pages)
    if search_results:
        print("\nSearch Results:")
        for idx, result in enumerate(search_results, start=1):
            print(f"{idx}. {result}")
    else:
        print("No search results found.")

if __name__ == "__main__":
    main()
